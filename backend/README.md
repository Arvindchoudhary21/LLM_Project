# Backend

step 1: Set up the project

Created a Python project folder named backend

Created core files:

main.py -> (FastAPI app)

qdrant_utils.py -> (Qdrant setup & insert)

search.py -> (semantic search function)

embeddings.py -> (embedding model)

data_loader.py -> (to uplaod the data to the qdrant database)

Step 2: Installed All Packages from requirements.txt  
Instead of installing each manually with pip install, we used:

pip install -r requirements.txt

This command:  
â€¢	Reads each line in requirements.txt  
â€¢	Installs all packages (and their dependencies)  
â€¢	Ensures clean, consistent setup on any machine  

âœ… fastapi  
Purpose:  
Used to build the backend API â€” for example:  
â€¢	/search endpoint for intelligent search  
â€¢	Handling HTTP requests/responses  
â€¢	Creating routes for search, health check, etc.  
Why we chose it:  
Fast, modern, async-ready, and Pythonic â€” perfect for REST APIs.  

âœ… uvicorn  
Purpose:  
This is the ASGI server used to run the FastAPI app.  
Command used:  
uvicorn main:app --reload  

Why we chose it:  
Required to serve FastAPI with live reloading in development.  

âœ… qdrant-client  
Purpose:  
Used to connect with the Qdrant vector database, including:  
â€¢	Creating a collection (biology-chapters)  
â€¢	Uploading embeddings to Qdrant  
â€¢	Performing semantic search with search() API  

Why we chose it:  
Itâ€™s the official Python client for Qdrant.  

âœ… sentence-transformers  
Purpose:  
To generate semantic vector embeddings from chapter content using the model:  
all-MiniLM-L6-v2  
Example:  
from sentence_transformers import SentenceTransformer  
model = SentenceTransformer('all-MiniLM-L6-v2')  
embedding = model.encode(text)  

Why we chose it:  
Lightweight, fast, accurate â€” works well for sentence-level embeddings.  


âœ… 1. Created the FastAPI App (main.py).

We initialized a FastAPI app to serve our API routes.

âœ… 2. Loaded SentenceTransformer (embeddings.py)

We used sentence-transformers and the model: all-MiniLM-L6-v2 to convert text to vector embeddings.

âœ… 3. Created qdrant_client.py

âœ… Purpose of this file:  
â€¢	Connects to Qdrant  
â€¢	Recreates the "biology_11" collection with vector size 384 and COSINE distance  
â€¢	Used before inserting embeddings into the DB.  

âœ… 4. Loaded and Inserted Data (data_loader.py)

This loaded your chapters or definitions and inserted embeddings.

âœ… 5. Created search.py

âœ… This handled the intelligent semantic search:  
â€¢	Embeds the user query  
â€¢	Searches Qdrant using client.search  
â€¢	Returns title, previewed content, and score  

âœ… 6. Connected search.py in FastAPI (main.py)

We called search_chapters() inside /search endpoint and returned top matches.

âœ… 7. Ran the API Server

uvicorn main:app --reload

And you tested via Postman or browser at:

http://localhost:8000/search



# How sentence-transformers/all-MiniLM-L6-v2 working

ğŸ§  âœ… LLM in Your Project: What Itâ€™s Doing  
The LLM is used to enable intelligent search â€” so when a user types a query (like â€œgenusâ€ or â€œwhat is taxonomyâ€), the system doesn't just look for exact words, but understands the meaning of the query and returns the most semantically relevant answer from the database.  

1. Backend Prepares the Data  
âœ… We have:  

â€¢	A file definitions.json or chapters.json with title + content  
â€¢	Each text is converted into a vector (embedding) using a pretrained LLM-based model all-MiniLM-L6-v2.

ğŸ“Œ Model Details:  

Feature	Description  

Name	                sentence-transformers/all-MiniLM-L6-v2  
Type	                Transformer-based (LLM-powered embedding model)  
Vector Size	            384 dimensions  
Training Objective	    Trained on semantic similarity and sentence-pair tasks  
Speed	                Very fast and lightweight (ideal for local use)  
From	                Hugging Face / Microsoft (via sentence-transformers)  
Library Used	        sentence-transformers Python library  

ğŸ” What It Does  
This model is not a chat-style LLM like GPT, but it is built using transformer layers, making it part of the LLM family of models.  
It is specially fine-tuned to:  
â€¢	Convert any sentence, paragraph, or short document  
â€¢	Into a fixed-size semantic embedding vector  
â€¢	Which can be compared using cosine similarity  

âœ… Why it's a Good Choice  
â€¢	Lightweight (runs locally)  
â€¢	Fast to load and process  
â€¢	Delivers great performance for semantic search  
â€¢	Works perfectly with Qdrant vector database  

2. Embeddings Go Into Qdrant  
â€¢	Those vectors are stored in a vector database: Qdrant  
â€¢	Each vector is tied to a title, content, and chapter_id  

3. User Types a Query  
â€¢	The query (e.g., â€œwhat is genus?â€) is also sent to the same embedding model â†’ converted to a vector  

4. Qdrant Does Vector Similarity Search  
Qdrant compares the query vector to all the stored vectors using cosine similarity.  
ğŸ” It finds the most similar vector (i.e., content semantically close to the userâ€™s question)  

5. Backend Sends Top Result to Frontend  
â€¢	The top result (title + content) is returned from FastAPI  
â€¢	Displayed on frontend in a ChapterCard component  

ğŸ“Œ Example  
âŒ Without LLM:  
â€¢	Search "what is genus" â†’ it would match only chapters containing the exact word "genus"  
âœ… With LLM:  
â€¢	Search "group of related species" â†’ it would still match the definition of genus, because the model understands the meaning  




