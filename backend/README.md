# Backend

step 1: Set up the project

Created a Python project folder named backend

Created core files:

main.py -> (FastAPI app)

qdrant_utils.py -> (Qdrant setup & insert)

search.py -> (semantic search function)

embeddings.py -> (embedding model)

data_loader.py -> (to uplaod the data to the qdrant database)

Step 2: Installed All Packages from requirements.txt  
Instead of installing each manually with pip install, we used:

pip install -r requirements.txt

This command:  
‚Ä¢	Reads each line in requirements.txt  
‚Ä¢	Installs all packages (and their dependencies)  
‚Ä¢	Ensures clean, consistent setup on any machine  

‚úÖ fastapi
Purpose:
Used to build the backend API ‚Äî for example:
‚Ä¢	/search endpoint for intelligent search
‚Ä¢	Handling HTTP requests/responses
‚Ä¢	Creating routes for search, health check, etc.
Why we chose it:
Fast, modern, async-ready, and Pythonic ‚Äî perfect for REST APIs.

‚úÖ uvicorn
Purpose:
This is the ASGI server used to run the FastAPI app.
Command used:
uvicorn main:app --reload

Why we chose it:
Required to serve FastAPI with live reloading in development.

‚úÖ qdrant-client
Purpose:
Used to connect with the Qdrant vector database, including:
‚Ä¢	Creating a collection (biology-chapters)
‚Ä¢	Uploading embeddings to Qdrant
‚Ä¢	Performing semantic search with search() API

Why we chose it:
It‚Äôs the official Python client for Qdrant.

‚úÖ sentence-transformers
Purpose:
To generate semantic vector embeddings from chapter content using the model:
all-MiniLM-L6-v2
Example:
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
embedding = model.encode(text)

Why we chose it:
Lightweight, fast, accurate ‚Äî works well for sentence-level embeddings.


‚úÖ 1. Created the FastAPI App (main.py).

We initialized a FastAPI app to serve our API routes.

‚úÖ 2. Loaded SentenceTransformer (embeddings.py)

We used sentence-transformers and the model: all-MiniLM-L6-v2 to convert text to vector embeddings.

‚úÖ 3. Created qdrant_client.py

‚úÖ Purpose of this file:
‚Ä¢	Connects to Qdrant
‚Ä¢	Recreates the "biology_11" collection with vector size 384 and COSINE distance
‚Ä¢	Used before inserting embeddings into the DB.

‚úÖ 4. Loaded and Inserted Data (data_loader.py)

This loaded your chapters or definitions and inserted embeddings.

‚úÖ 5. Created search.py

‚úÖ This handled the intelligent semantic search:
‚Ä¢	Embeds the user query
‚Ä¢	Searches Qdrant using client.search
‚Ä¢	Returns title, previewed content, and score

‚úÖ 6. Connected search.py in FastAPI (main.py)

We called search_chapters() inside /search endpoint and returned top matches.

‚úÖ 7. Ran the API Server

uvicorn main:app --reload

And you tested via Postman or browser at:

http://localhost:8000/search



# How sentence-transformers/all-MiniLM-L6-v2 working

üß† ‚úÖ LLM in Your Project: What It‚Äôs Doing
The LLM is used to enable intelligent search ‚Äî so when a user types a query (like ‚Äúgenus‚Äù or ‚Äúwhat is taxonomy‚Äù), the system doesn't just look for exact words, but understands the meaning of the query and returns the most semantically relevant answer from the database.

1. Backend Prepares the Data
‚úÖ We have:

‚Ä¢	A file definitions.json or chapters.json with title + content
‚Ä¢	Each text is converted into a vector (embedding) using a pretrained LLM-based model all-MiniLM-L6-v2.

üìå Model Details:

Feature	Description

Name	                sentence-transformers/all-MiniLM-L6-v2
Type	                Transformer-based (LLM-powered embedding model)
Vector Size	            384 dimensions
Training Objective	    Trained on semantic similarity and sentence-pair tasks
Speed	                Very fast and lightweight (ideal for local use)
From	                Hugging Face / Microsoft (via sentence-transformers)
Library Used	        sentence-transformers Python library

üîç What It Does
This model is not a chat-style LLM like GPT, but it is built using transformer layers, making it part of the LLM family of models.
It is specially fine-tuned to:
‚Ä¢	Convert any sentence, paragraph, or short document
‚Ä¢	Into a fixed-size semantic embedding vector
‚Ä¢	Which can be compared using cosine similarity

‚úÖ Why it's a Good Choice
‚Ä¢	Lightweight (runs locally)
‚Ä¢	Fast to load and process
‚Ä¢	Delivers great performance for semantic search
‚Ä¢	Works perfectly with Qdrant vector database

2. What LLM Are You Using?
We are using the all-MiniLM-L6-v2 model from Hugging Face via sentence-transformers.
This model is not a full GPT-type LLM, but a sentence-level transformer trained to:
‚ú® Convert a piece of text into a fixed-size vector that captures its meaning
So it‚Äôs LLM-powered embeddings, not full natural language generation.

3. Embeddings Go Into Qdrant
‚Ä¢	Those vectors are stored in a vector database: Qdrant
‚Ä¢	Each vector is tied to a title, content, and chapter_id

4. User Types a Query
‚Ä¢	The query (e.g., ‚Äúwhat is genus?‚Äù) is also sent to the same embedding model ‚Üí converted to a vector

5. Qdrant Does Vector Similarity Search
Qdrant compares the query vector to all the stored vectors using cosine similarity.
üîç It finds the most similar vector (i.e., content semantically close to the user‚Äôs question)

6. Backend Sends Top Result to Frontend
‚Ä¢	The top result (title + content) is returned from FastAPI
‚Ä¢	Displayed on frontend in a ChapterCard component

üìå Example
‚ùå Without LLM:
‚Ä¢	Search "what is genus" ‚Üí it would match only chapters containing the exact word "genus"
‚úÖ With LLM:
‚Ä¢	Search "group of related species" ‚Üí it would still match the definition of genus, because the model understands the meaning




